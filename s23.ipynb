{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "from PIL import ImageEnhance\n",
        "import random\n",
        "import os\n",
        "import io\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "# Install required packages\n",
        "!pip install transformers\n",
        "\n",
        "# Import the pipeline after installation\n",
        "from transformers import pipeline\n",
        "\n",
        "# Download CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Prepare dataset of the first 10 images\n",
        "subset_x = x_train[:10]\n",
        "subset_y = y_train[:10]\n",
        "\n",
        "# Print shapes of the subset to verify\n",
        "print(\"Subset of Images shape:\", subset_x.shape)\n",
        "print(\"Subset of Labels shape:\", subset_y.shape)\n",
        "\n",
        "# Initialize the image captioning pipeline with BLIP\n",
        "image_captioning_pipeline = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "# CIFAR-10 class names for better filenames\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Create a folder to store our images and descriptions\n",
        "!mkdir -p image_descriptions\n",
        "\n",
        "# Create a temporary directory for files that will go into the zip\n",
        "!mkdir -p temp_for_zip\n",
        "\n",
        "# Function to apply different augmentations to an image\n",
        "def augment_image(image, augmentation_type):\n",
        "    modified_image = image.copy()\n",
        "\n",
        "    if augmentation_type == 0:\n",
        "        # Original image\n",
        "        return modified_image\n",
        "    elif augmentation_type == 1:\n",
        "        # Slightly rotated\n",
        "        return modified_image.rotate(10)\n",
        "    elif augmentation_type == 2:\n",
        "        # Slightly cropped\n",
        "        width, height = modified_image.size\n",
        "        crop_size = int(min(width, height) * 0.9)\n",
        "        left = (width - crop_size) // 2\n",
        "        top = (height - crop_size) // 2\n",
        "        modified_image = modified_image.crop((left, top, left + crop_size, top + crop_size))\n",
        "        return modified_image.resize((width, height))\n",
        "    elif augmentation_type == 3:\n",
        "        # Adjust brightness\n",
        "        enhancer = ImageEnhance.Brightness(modified_image)\n",
        "        return enhancer.enhance(1.2)\n",
        "    else:\n",
        "        # Adjust contrast\n",
        "        enhancer = ImageEnhance.Contrast(modified_image)\n",
        "        return enhancer.enhance(1.2)\n",
        "\n",
        "# Create a ZIP file\n",
        "zip_buffer = io.BytesIO()\n",
        "with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n",
        "\n",
        "    # Process each image\n",
        "    for i in range(len(subset_x)):\n",
        "        # Get class name from the label for better naming\n",
        "        class_idx = subset_y[i][0]\n",
        "        class_name = class_names[class_idx]\n",
        "\n",
        "        # Create a meaningful filename\n",
        "        image_filename = f\"image_{i+1}_{class_name}.png\"\n",
        "\n",
        "        # Convert numpy array to PIL Image\n",
        "        image = PIL.Image.fromarray(subset_x[i])\n",
        "\n",
        "        # Save the original image\n",
        "        image_path = f\"temp_for_zip/{image_filename}\"\n",
        "        image.save(image_path)\n",
        "\n",
        "        # Add image to zip file\n",
        "        zip_file.write(image_path, arcname=image_filename)\n",
        "\n",
        "        # Create a text file for descriptions\n",
        "        descriptions_filename = f\"image_{i+1}_{class_name}_descriptions.txt\"\n",
        "        descriptions_path = f\"temp_for_zip/{descriptions_filename}\"\n",
        "\n",
        "        with open(descriptions_path, 'w') as desc_file:\n",
        "            desc_file.write(f\"Descriptions for {image_filename}:\\n\\n\")\n",
        "\n",
        "            # Generate 5 different descriptions using image augmentation\n",
        "            for j in range(5):\n",
        "                # Apply different augmentation for each description\n",
        "                modified_image = augment_image(image, j)\n",
        "\n",
        "                # Generate caption\n",
        "                caption_output = image_captioning_pipeline(modified_image)\n",
        "                caption = caption_output[0][\"generated_text\"]\n",
        "\n",
        "                # Write to file\n",
        "                desc_file.write(f\"Description {j+1}: {caption}\\n\")\n",
        "\n",
        "                # Also print to console\n",
        "                print(f\"Image {i+1} ({class_name}) - Description {j+1}: {caption}\")\n",
        "\n",
        "        # Add descriptions file to zip\n",
        "        zip_file.write(descriptions_path, arcname=descriptions_filename)\n",
        "\n",
        "        # Display the image in the notebook\n",
        "        plt.figure(figsize=(3, 3))\n",
        "        plt.imshow(subset_x[i])\n",
        "        plt.title(f\"Image {i+1}: {class_name}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "# Download the zip file\n",
        "zip_buffer.seek(0)\n",
        "with open('image_descriptions.zip', 'wb') as f:\n",
        "    f.write(zip_buffer.getvalue())\n",
        "\n",
        "# Clean up temporary files\n",
        "!rm -rf temp_for_zip\n",
        "\n",
        "# Download the zip file to the user's computer\n",
        "files.download('image_descriptions.zip')\n",
        "\n",
        "print(\"\\nProcess complete! The zip file has been downloaded to your computer.\")\n",
        "print(\"The zip contains 10 images and 10 text files with 5 descriptions for each image.\")"
      ],
      "metadata": {
        "id": "1tRF-Qe34LE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a SigLIP model on CIFAR-10 images with descriptions\n",
        "# For Google Colab execution - Using the ZIP file from the previous cell\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "# Install required packages\n",
        "!pip install torch torchvision transformers tqdm matplotlib\n",
        "\n",
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Step 1: Use the zip file already generated from the previous cell\n",
        "zip_path = \"image_descriptions.zip\"\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(zip_path):\n",
        "    raise FileNotFoundError(f\"Could not find {zip_path}. Make sure the previous cell completed successfully.\")\n",
        "\n",
        "print(f\"Found existing zip file: {zip_path}\")\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"dataset\")\n",
        "    print(\"Extracted files to 'dataset' folder\")\n",
        "\n",
        "# Step 2: Create Dataset class to load images and descriptions\n",
        "class ImageTextDataset(Dataset):\n",
        "    def __init__(self, folder_path, transform=None):\n",
        "        self.folder_path = folder_path\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "\n",
        "        # Find all image files\n",
        "        image_files = [f for f in os.listdir(folder_path) if f.endswith('.png')]\n",
        "\n",
        "        # For each image, find its descriptions\n",
        "        for img_file in image_files:\n",
        "            # Get base name without extension\n",
        "            base_name = img_file.rsplit('.', 1)[0]\n",
        "            desc_file = f\"{base_name}_descriptions.txt\"\n",
        "            desc_path = os.path.join(folder_path, desc_file)\n",
        "\n",
        "            if os.path.exists(desc_path):\n",
        "                # Read descriptions\n",
        "                with open(desc_path, 'r') as f:\n",
        "                    lines = f.readlines()\n",
        "\n",
        "                # Extract descriptions (skip header lines)\n",
        "                descriptions = []\n",
        "                for line in lines:\n",
        "                    match = re.search(r'Description \\d+: (.*)', line)\n",
        "                    if match:\n",
        "                        descriptions.append(match.group(1))\n",
        "\n",
        "                # Add image-text pairs to samples\n",
        "                for desc in descriptions:\n",
        "                    self.samples.append((img_file, desc))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_file, text = self.samples[idx]\n",
        "        img_path = os.path.join(self.folder_path, img_file)\n",
        "\n",
        "        # Load and transform image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, text\n",
        "\n",
        "# Step 3: Create the SigLIP model architecture\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, output_dim=512):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        # Use a pretrained BERT model\n",
        "        self.bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "        # Add a projection layer\n",
        "        self.projection = nn.Linear(768, output_dim)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # Use [CLS] token representation\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
        "        # Project to output dimension\n",
        "        return self.projection(cls_output)\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self, output_dim=512):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        # Use a pretrained ResNet model\n",
        "        weights = ResNet18_Weights.DEFAULT\n",
        "        self.resnet = resnet18(weights=weights)\n",
        "        # Replace the final FC layer\n",
        "        self.resnet.fc = nn.Linear(512, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "class SigLIP(nn.Module):\n",
        "    def __init__(self, output_dim=512):\n",
        "        super(SigLIP, self).__init__()\n",
        "        self.image_encoder = ImageEncoder(output_dim)\n",
        "        self.text_encoder = TextEncoder(output_dim)\n",
        "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
        "\n",
        "    def forward(self, images, input_ids, attention_mask):\n",
        "        # Encode images and text\n",
        "        image_features = self.image_encoder(images)\n",
        "        text_features = self.text_encoder(input_ids, attention_mask)\n",
        "\n",
        "        # Normalize features\n",
        "        image_features = F.normalize(image_features, dim=1)\n",
        "        text_features = F.normalize(text_features, dim=1)\n",
        "\n",
        "        # Compute similarity matrix\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits = logit_scale * image_features @ text_features.t()\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Step 4: Define the SigLIP loss function\n",
        "class SigLIPLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SigLIPLoss, self).__init__()\n",
        "\n",
        "    def forward(self, logits):\n",
        "        # Create labels (diagonal should be 1, rest 0)\n",
        "        labels = torch.eye(logits.shape[0], device=logits.device)\n",
        "\n",
        "        # Apply sigmoid loss\n",
        "        loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
        "\n",
        "        return loss\n",
        "\n",
        "# Step 5: Set up the training loop\n",
        "def train_siglip(model, train_loader, optimizer, criterion, tokenizer, epochs=10):\n",
        "    model.train()\n",
        "    training_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for batch_idx, (images, texts) in enumerate(progress_bar):\n",
        "            # Tokenize texts\n",
        "            encodings = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "            input_ids = encodings['input_ids'].to(device)\n",
        "            attention_mask = encodings['attention_mask'].to(device)\n",
        "\n",
        "            # Move images to device\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(images, input_ids, attention_mask)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(logits)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss.item()\n",
        "            progress_bar.set_postfix(loss=running_loss/(batch_idx+1))\n",
        "\n",
        "        epoch_loss = running_loss/len(train_loader)\n",
        "        training_losses.append(epoch_loss)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.6f}\")\n",
        "\n",
        "    # Plot the training loss\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, epochs+1), training_losses, marker='o')\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Step 6: Function to test image retrieval\n",
        "def test_image_retrieval(model, dataset, tokenizer):\n",
        "    model.eval()\n",
        "\n",
        "    # Create dataloaders for all images and texts\n",
        "    all_images = []\n",
        "    all_texts = []\n",
        "    unique_images = {}\n",
        "    image_files_list = []\n",
        "\n",
        "    for img_file, text in dataset.samples:\n",
        "        if img_file not in unique_images:\n",
        "            img_path = os.path.join(dataset.folder_path, img_file)\n",
        "            image = dataset.transform(Image.open(img_path).convert('RGB'))\n",
        "            all_images.append(image)\n",
        "            unique_images[img_file] = len(all_images) - 1\n",
        "            image_files_list.append(img_file)\n",
        "\n",
        "        all_texts.append(text)\n",
        "\n",
        "    # Convert to tensors\n",
        "    all_images = torch.stack(all_images).to(device)\n",
        "\n",
        "    # Encode all images\n",
        "    with torch.no_grad():\n",
        "        image_features = model.image_encoder(all_images)\n",
        "        image_features = F.normalize(image_features, dim=1)\n",
        "\n",
        "    # Test retrieval for a few texts\n",
        "    test_texts = all_texts[:5]  # Just test the first 5 texts\n",
        "\n",
        "    print(\"\\nTesting image retrieval:\")\n",
        "    for i, text in enumerate(test_texts):\n",
        "        # Tokenize text\n",
        "        encodings = tokenizer([text], padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        input_ids = encodings['input_ids'].to(device)\n",
        "        attention_mask = encodings['attention_mask'].to(device)\n",
        "\n",
        "        # Encode text\n",
        "        with torch.no_grad():\n",
        "            text_feature = model.text_encoder(input_ids, attention_mask)\n",
        "            text_feature = F.normalize(text_feature, dim=1)\n",
        "\n",
        "        # Compute similarities\n",
        "        logit_scale = model.logit_scale.exp()\n",
        "        similarities = logit_scale * (text_feature @ image_features.t()).squeeze()\n",
        "\n",
        "        # Get top 3 matches\n",
        "        top_matches = torch.argsort(similarities, descending=True)[:3].tolist()\n",
        "\n",
        "        print(f\"\\nFor text {i+1}: '{text}'\")\n",
        "        for rank, idx in enumerate(top_matches):\n",
        "            img_file = image_files_list[idx]\n",
        "            score = similarities[idx].item()\n",
        "            print(f\"  Match {rank+1}: {img_file} (score: {score:.4f})\")\n",
        "\n",
        "            # Display the image\n",
        "            img_path = os.path.join(dataset.folder_path, img_file)\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            plt.figure(figsize=(3, 3))\n",
        "            plt.imshow(np.array(img))\n",
        "            plt.title(f\"Match {rank+1}: {img_file}\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "# Step 7: Test text retrieval given an image\n",
        "def test_text_retrieval(model, dataset, tokenizer):\n",
        "    model.eval()\n",
        "\n",
        "    # Encode all texts\n",
        "    all_texts = []\n",
        "    all_image_files = []\n",
        "    unique_texts = {}\n",
        "\n",
        "    for img_file, text in dataset.samples:\n",
        "        all_texts.append(text)\n",
        "        all_image_files.append(img_file)\n",
        "\n",
        "    # Tokenize all texts\n",
        "    encodings = tokenizer(all_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    input_ids = encodings['input_ids'].to(device)\n",
        "    attention_mask = encodings['attention_mask'].to(device)\n",
        "\n",
        "    # Encode all texts\n",
        "    with torch.no_grad():\n",
        "        text_features = model.text_encoder(input_ids, attention_mask)\n",
        "        text_features = F.normalize(text_features, dim=1)\n",
        "\n",
        "    # Test retrieval for a few images\n",
        "    test_image_indices = [0, 10, 20, 30, 40]  # Test a few images\n",
        "\n",
        "    print(\"\\nTesting text retrieval:\")\n",
        "    for idx in test_image_indices:\n",
        "        if idx >= len(dataset):\n",
        "            continue\n",
        "\n",
        "        image, _ = dataset[idx]\n",
        "        img_file = all_image_files[idx]\n",
        "\n",
        "        # Display the query image\n",
        "        img_path = os.path.join(dataset.folder_path, img_file)\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        plt.figure(figsize=(3, 3))\n",
        "        plt.imshow(np.array(img))\n",
        "        plt.title(f\"Query Image: {img_file}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        # Encode image\n",
        "        image = image.unsqueeze(0).to(device)  # Add batch dimension\n",
        "        with torch.no_grad():\n",
        "            image_feature = model.image_encoder(image)\n",
        "            image_feature = F.normalize(image_feature, dim=1)\n",
        "\n",
        "        # Compute similarities\n",
        "        logit_scale = model.logit_scale.exp()\n",
        "        similarities = logit_scale * (image_feature @ text_features.t()).squeeze()\n",
        "\n",
        "        # Get top 3 matches\n",
        "        top_matches = torch.argsort(similarities, descending=True)[:3].tolist()\n",
        "\n",
        "        print(f\"\\nFor image: {img_file}\")\n",
        "        for rank, text_idx in enumerate(top_matches):\n",
        "            matched_text = all_texts[text_idx]\n",
        "            score = similarities[text_idx].item()\n",
        "            print(f\"  Match {rank+1}: '{matched_text}' (score: {score:.4f})\")\n",
        "\n",
        "# Step 8: Main function to run the training\n",
        "def main():\n",
        "    # Set up transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    dataset = ImageTextDataset(folder_path=\"dataset\", transform=transform)\n",
        "    print(f\"Dataset size: {len(dataset)} image-text pairs\")\n",
        "\n",
        "    # Display a few examples\n",
        "    print(\"\\nSample image-text pairs:\")\n",
        "    for i in range(min(5, len(dataset))):\n",
        "        print(f\"Example {i+1}: {dataset.samples[i][0]} - {dataset.samples[i][1]}\")\n",
        "\n",
        "    # Create dataloader\n",
        "    batch_size = 1  # Small batch size for this tiny dataset\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = SigLIP(output_dim=512).to(device)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    # Initialize loss function\n",
        "    criterion = SigLIPLoss()\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\nStarting SigLIP training...\")\n",
        "    model = train_siglip(model, train_loader, optimizer, criterion, tokenizer, epochs=10)\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), \"siglip_cifar10.pth\")\n",
        "    print(\"Model saved to siglip_cifar10.pth\")\n",
        "\n",
        "    # Test the model on a few examples\n",
        "    test_image_retrieval(model, dataset, tokenizer)\n",
        "    test_text_retrieval(model, dataset, tokenizer)\n",
        "\n",
        "    print(\"\\nTraining and evaluation complete!\")\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "aPMGIN2W_5sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision transformers tqdm matplotlib peft bitsandbytes accelerate wandb\n"
      ],
      "metadata": {
        "id": "kIa5Nt09fVdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes # Ensure bitsandbytes is installed/updated\n",
        "# Training a VLM (Vision + Phi3) with SigLIP vision encoder and frozen Phi-3 text decoder\n",
        "# Optimized for smaller model size with QLoRA\n",
        "# For Google Colab execution\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModel\n",
        "import bitsandbytes as bnb # Import the bitsandbytes library\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
        "import logging\n",
        "import wandb\n",
        "from datetime import datetime\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Install required packages\n",
        "!pip install torch torchvision transformers tqdm matplotlib peft bitsandbytes accelerate wandb\n",
        "\n",
        "# ... (Rest of the code)"
      ],
      "metadata": {
        "id": "GbtTK-zDaZ_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a VLM (Vision + Phi3) with SigLIP vision encoder and frozen Phi-3 text decoder\n",
        "# Optimized for smaller model size with QLoRA\n",
        "# For Google Colab execution\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModel\n",
        "import bitsandbytes as bnb\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
        "import logging\n",
        "import wandb\n",
        "from datetime import datetime\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Install required packages\n",
        "!pip install torch torchvision transformers tqdm matplotlib peft bitsandbytes accelerate wandb\n",
        "\n",
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info(f\"Using device: {device}\")\n",
        "\n",
        "# Check if the SigLIP model file exists\n",
        "siglip_path = \"siglip_cifar10.pth\"\n",
        "if not os.path.exists(siglip_path):\n",
        "    raise FileNotFoundError(f\"Could not find {siglip_path}. Make sure the model file is available.\")\n",
        "\n",
        "logger.info(f\"Found SigLIP model file: {siglip_path}\")\n",
        "\n",
        "# Check if the zip file exists (in case we need to use the dataset again)\n",
        "zip_path = \"image_descriptions.zip\"\n",
        "if not os.path.exists(zip_path):\n",
        "    raise FileNotFoundError(f\"Could not find {zip_path}. Make sure the dataset file is available.\")\n",
        "\n",
        "# Extract the zip file if the dataset folder doesn't exist\n",
        "if not os.path.exists(\"dataset\"):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"dataset\")\n",
        "        logger.info(\"Extracted files to 'dataset' folder\")\n",
        "\n",
        "# Define model classes from the previous code\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, output_dim=512):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.projection = nn.Linear(768, output_dim)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
        "        return self.projection(cls_output)\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self, output_dim=512):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        from torchvision.models import resnet18, ResNet18_Weights\n",
        "        weights = ResNet18_Weights.DEFAULT\n",
        "        self.resnet = resnet18(weights=weights)\n",
        "        self.resnet.fc = nn.Linear(512, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "class SigLIP(nn.Module):\n",
        "    def __init__(self, output_dim=512):\n",
        "        super(SigLIP, self).__init__()\n",
        "        self.image_encoder = ImageEncoder(output_dim)\n",
        "        self.text_encoder = TextEncoder(output_dim)\n",
        "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
        "\n",
        "    def forward(self, images, input_ids=None, attention_mask=None):\n",
        "        # If text inputs are provided, compute full SigLIP forward pass\n",
        "        if input_ids is not None and attention_mask is not None:\n",
        "            image_features = self.image_encoder(images)\n",
        "            text_features = self.text_encoder(input_ids, attention_mask)\n",
        "\n",
        "            image_features = F.normalize(image_features, dim=1)\n",
        "            text_features = F.normalize(text_features, dim=1)\n",
        "\n",
        "            logit_scale = self.logit_scale.exp()\n",
        "            logits = logit_scale * image_features @ text_features.t()\n",
        "\n",
        "            return logits\n",
        "        # Otherwise, just return image features (for VLM)\n",
        "        else:\n",
        "            return self.image_encoder(images)\n",
        "\n",
        "# Create VLM dataset class\n",
        "class VLMDataset(Dataset):\n",
        "    def __init__(self, folder_path, transform=None, max_length=512):\n",
        "        self.folder_path = folder_path\n",
        "        self.transform = transform\n",
        "        self.max_length = max_length\n",
        "        self.samples = []\n",
        "\n",
        "        # Find all image files\n",
        "        image_files = [f for f in os.listdir(folder_path) if f.endswith('.png')]\n",
        "\n",
        "        # For each image, find its descriptions\n",
        "        for img_file in image_files:\n",
        "            # Get base name without extension\n",
        "            base_name = img_file.rsplit('.', 1)[0]\n",
        "            desc_file = f\"{base_name}_descriptions.txt\"\n",
        "            desc_path = os.path.join(folder_path, desc_file)\n",
        "\n",
        "            if os.path.exists(desc_path):\n",
        "                # Read descriptions\n",
        "                with open(desc_path, 'r') as f:\n",
        "                    lines = f.readlines()\n",
        "\n",
        "                # Extract descriptions (skip header lines)\n",
        "                descriptions = []\n",
        "                for line in lines:\n",
        "                    match = re.search(r'Description \\d+: (.*)', line)\n",
        "                    if match:\n",
        "                        descriptions.append(match.group(1))\n",
        "\n",
        "                # Add image-text pairs to samples\n",
        "                for desc in descriptions:\n",
        "                    self.samples.append((img_file, desc))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_file, text = self.samples[idx]\n",
        "        img_path = os.path.join(self.folder_path, img_file)\n",
        "\n",
        "        # Load and transform image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, text\n",
        "\n",
        "# Define the Vision-Language Model\n",
        "class VLM(nn.Module):\n",
        "    def __init__(self, vision_encoder, text_decoder, projection_dim=4096):\n",
        "        super(VLM, self).__init__()\n",
        "        self.vision_encoder = vision_encoder\n",
        "        self.text_decoder = text_decoder\n",
        "\n",
        "        # Projection layer to map vision embeddings to text model dimension\n",
        "        vision_dim = 512  # SigLIP output dimension\n",
        "        text_dim = self.text_decoder.config.hidden_size\n",
        "        self.projection = nn.Linear(vision_dim, text_dim)\n",
        "\n",
        "    def forward(self, images, input_ids, attention_mask):\n",
        "        # Encode images\n",
        "        with torch.no_grad():  # Freeze vision encoder\n",
        "            image_features = self.vision_encoder(images)\n",
        "\n",
        "        # Project image features to text model dimension\n",
        "        projected_features = self.projection(image_features)\n",
        "\n",
        "        # Prepare for text decoder (reshape to match hidden states)\n",
        "        batch_size = images.shape[0]\n",
        "\n",
        "        # Get outputs from text decoder\n",
        "        outputs = self.text_decoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            inputs_embeds=None,  # We'll use the standard token embeddings\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "\n",
        "        # For training, we use the language modeling head to predict the next tokens\n",
        "        logits = outputs.logits\n",
        "\n",
        "        return logits\n",
        "\n",
        "def get_qlora_config():\n",
        "    return LoraConfig(\n",
        "        r=4,  # Reduced rank for smaller size\n",
        "        lora_alpha=16,\n",
        "        # Updated module names for Phi-3 model\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "\n",
        "# Initialize the models and training components\n",
        "def initialize_models():\n",
        "    # 1. Load the trained SigLIP vision encoder\n",
        "    vision_encoder = SigLIP(output_dim=512)\n",
        "    vision_encoder.load_state_dict(torch.load(siglip_path, map_location=device))\n",
        "    vision_encoder = vision_encoder.to(device)\n",
        "    vision_encoder.eval()  # Freeze the vision encoder\n",
        "    logger.info(\"SigLIP vision encoder loaded successfully\")\n",
        "\n",
        "    # 2. Initialize Phi-3 with BitsAndBytes for quantization\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    # Use the Microsoft Phi-3 mini model\n",
        "    model_id = \"microsoft/phi-3-mini-4k-instruct\"\n",
        "\n",
        "    # Load the tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    text_decoder = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Prepare model for k-bit training\n",
        "    text_decoder = prepare_model_for_kbit_training(text_decoder)\n",
        "\n",
        "    # Apply optimized LoRA\n",
        "    peft_config = get_qlora_config()\n",
        "    text_decoder = get_peft_model(text_decoder, peft_config)\n",
        "\n",
        "    # Freeze the base model parameters\n",
        "    for param in text_decoder.base_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # 3. Create the VLM model\n",
        "    vlm_model = VLM(vision_encoder, text_decoder)\n",
        "\n",
        "    # Move to device\n",
        "    vlm_model = vlm_model.to(device)\n",
        "\n",
        "    # Count trainable parameters\n",
        "    trainable_params = sum(p.numel() for p in vlm_model.parameters() if p.requires_grad)\n",
        "    logger.info(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    logger.info(f\"Total parameters: {sum(p.numel() for p in vlm_model.parameters()):,}\")\n",
        "\n",
        "    return vlm_model, tokenizer\n",
        "\n",
        "# Training function with logging\n",
        "def train_vlm(model, train_loader, optimizer, tokenizer, epochs=5):\n",
        "    # Initialize wandb for logging\n",
        "    run = wandb.init(project=\"vlm-training\", name=f\"vlm-siglip-phi3-{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = epochs * len(train_loader)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for batch_idx, (images, texts) in enumerate(progress_bar):\n",
        "            # Prepare text inputs with special format for instruction-based models\n",
        "            formatted_texts = []\n",
        "            for text in texts:\n",
        "                formatted_text = f\"<|user|>\\nDescribe this image.\\n<|assistant|>\\n{text}</s>\"\n",
        "                formatted_texts.append(formatted_text)\n",
        "\n",
        "            # Tokenize texts - we'll use this both for inputs and targets\n",
        "            encodings = tokenizer(formatted_texts, padding=\"max_length\", truncation=True,\n",
        "                                 max_length=512, return_tensors=\"pt\")\n",
        "            input_ids = encodings['input_ids'].to(device)\n",
        "            attention_mask = encodings['attention_mask'].to(device)\n",
        "\n",
        "            # Prepare target labels (shifted input_ids for causal language modeling)\n",
        "            labels = input_ids.clone()\n",
        "\n",
        "            # Set labels to -100 for non-target tokens (padding and prompt tokens)\n",
        "            # This identifies which tokens are part of the response we want to train on\n",
        "            for i, text in enumerate(formatted_texts):\n",
        "                prompt_len = len(tokenizer(text.split(\"<|assistant|>\\n\")[0] + \"<|assistant|>\\n\",\n",
        "                                          add_special_tokens=False)['input_ids'])\n",
        "                labels[i, :prompt_len] = -100  # Ignore tokens up to the assistant's response\n",
        "\n",
        "            # Move images to device\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(images, input_ids, attention_mask)\n",
        "\n",
        "            # Compute loss\n",
        "            # We need to reshape as our logits are [batch_size, seq_len, vocab_size]\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "            # Compute accuracy\n",
        "            # Only consider positions where labels != -100\n",
        "            active_logits = shift_logits.view(-1, shift_logits.shape[-1])\n",
        "            active_labels = shift_labels.view(-1)\n",
        "            active_mask = active_labels != -100\n",
        "\n",
        "            if active_mask.sum() > 0:  # Only compute if we have valid positions\n",
        "                active_preds = torch.argmax(active_logits[active_mask], dim=-1)\n",
        "                active_labels_filtered = active_labels[active_mask]\n",
        "                correct = (active_preds == active_labels_filtered).sum().item()\n",
        "                total = active_mask.sum().item()\n",
        "\n",
        "                correct_predictions += correct\n",
        "                total_predictions += total\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix(\n",
        "                loss=running_loss/(batch_idx+1),\n",
        "                acc=correct_predictions/max(1, total_predictions)\n",
        "            )\n",
        "\n",
        "            # Log to wandb\n",
        "            step = epoch * len(train_loader) + batch_idx\n",
        "            wandb.log({\n",
        "                \"train/loss\": loss.item(),\n",
        "                \"train/accuracy\": correct/max(1, total) if active_mask.sum() > 0 else 0,\n",
        "                \"train/step\": step,\n",
        "                \"train/progress\": step/total_steps\n",
        "            })\n",
        "\n",
        "        # End of epoch\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_accuracy = correct_predictions / max(1, total_predictions)\n",
        "\n",
        "        logger.info(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.6f}, Accuracy: {epoch_accuracy:.4f}\")\n",
        "\n",
        "        # Log epoch metrics\n",
        "        wandb.log({\n",
        "            \"train/epoch\": epoch+1,\n",
        "            \"train/epoch_loss\": epoch_loss,\n",
        "            \"train/epoch_accuracy\": epoch_accuracy\n",
        "        })\n",
        "\n",
        "        # Save model checkpoint efficiently (only the necessary parts)\n",
        "        checkpoint_path = f\"efficient_vlm_checkpoint_epoch_{epoch+1}\"\n",
        "        save_efficient_model(model, tokenizer, path=checkpoint_path)\n",
        "        logger.info(f\"Efficient checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "    wandb.finish()\n",
        "    return model\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_vlm(model, tokenizer, eval_dataset, num_samples=5):\n",
        "    model.eval()\n",
        "\n",
        "    # Create a dataloader with batch size 1 for evaluation\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    logger.info(\"Generating image descriptions...\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Get the original dataset from the subset\n",
        "    original_dataset = eval_dataset.dataset\n",
        "\n",
        "    for i, (image, true_text) in enumerate(eval_loader):\n",
        "        if i >= num_samples:\n",
        "            break\n",
        "\n",
        "        image = image.to(device)\n",
        "\n",
        "        # Input prompt\n",
        "        prompt = \"<|user|>\\nDescribe this image.\\n<|assistant|>\\n\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "        input_ids = inputs.input_ids.to(device)\n",
        "        attention_mask = inputs.attention_mask.to(device)\n",
        "\n",
        "        # Generate text\n",
        "        with torch.no_grad():\n",
        "            # Get image features\n",
        "            image_features = model.vision_encoder(image)\n",
        "\n",
        "            # Project features\n",
        "            projected_features = model.projection(image_features)\n",
        "\n",
        "            # Generate text with the decoder\n",
        "            outputs = model.text_decoder.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=100,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                num_return_sequences=1\n",
        "            )\n",
        "\n",
        "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Extract just the assistant's response\n",
        "            if \"<|assistant|>\" in generated_text:\n",
        "                generated_text = generated_text.split(\"<|assistant|>\")[1].strip()\n",
        "\n",
        "        # Display results\n",
        "        print(f\"\\nSample {i+1}:\")\n",
        "        print(f\"True description: {true_text[0]}\")\n",
        "        print(f\"Generated description: {generated_text}\")\n",
        "\n",
        "        # Display the image - fix access to the original dataset\n",
        "        # Get the index in the original dataset\n",
        "        original_idx = eval_dataset.indices[i]\n",
        "        img_path = os.path.join(original_dataset.folder_path, original_dataset.samples[original_idx][0])\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(np.array(img))\n",
        "        plt.title(\"Input Image\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        results.append({\n",
        "            \"image_path\": img_path,\n",
        "            \"true_text\": true_text[0],\n",
        "            \"generated_text\": generated_text\n",
        "        })\n",
        "\n",
        "    # Save results to file\n",
        "    with open(\"vlm_evaluation_results.json\", \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Function to save efficient model (only the necessary components)\n",
        "def save_efficient_model(model, tokenizer, path=\"efficient_vlm\"):\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "    # 1. Save the projection layer\n",
        "    torch.save(model.projection.state_dict(), f\"{path}/projection_layer.pt\")\n",
        "\n",
        "    # 2. Save LoRA weights only (not the full model)\n",
        "    model.text_decoder.save_pretrained(f\"{path}/lora_weights\")\n",
        "\n",
        "    # 3. Save tokenizer\n",
        "    tokenizer.save_pretrained(f\"{path}/tokenizer\")\n",
        "\n",
        "    # 4. Save a config file with architecture details\n",
        "    config = {\n",
        "        \"vision_model\": \"SigLIP\",\n",
        "        \"text_model\": \"microsoft/phi-3-mini-4k-instruct\",\n",
        "        \"projection_dim\": model.projection.out_features,\n",
        "        \"input_dim\": model.projection.in_features,\n",
        "        \"quantization\": \"4bit\",\n",
        "        \"lora_config\": {\n",
        "            \"r\": 4,\n",
        "            \"alpha\": 16,\n",
        "            \"target_modules\": [\"q_proj\", \"v_proj\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(f\"{path}/config.json\", \"w\") as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    logger.info(f\"Efficient model saved to {path}\")\n",
        "\n",
        "    # 5. Log the model size\n",
        "    total_size = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(path):\n",
        "        for f in filenames:\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            total_size += os.path.getsize(fp)\n",
        "\n",
        "    logger.info(f\"Total model size: {total_size / (1024 * 1024):.2f} MB\")\n",
        "\n",
        "# Function to load efficient model\n",
        "def load_efficient_model(path=\"efficient_vlm\"):\n",
        "    # Load config\n",
        "    with open(f\"{path}/config.json\", \"r\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    # Initialize vision encoder\n",
        "    vision_encoder = SigLIP(output_dim=config[\"input_dim\"])\n",
        "    vision_encoder.load_state_dict(torch.load(\"siglip_cifar10.pth\", map_location=device))\n",
        "    vision_encoder.eval()\n",
        "\n",
        "    # Initialize text decoder with quantization\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(f\"{path}/tokenizer\")\n",
        "\n",
        "    # Load text model\n",
        "    text_decoder = AutoModelForCausalLM.from_pretrained(\n",
        "        config[\"text_model\"],\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Load LoRA weights\n",
        "    text_decoder = PeftModel.from_pretrained(text_decoder, f\"{path}/lora_weights\")\n",
        "\n",
        "    # Create VLM\n",
        "    vlm_model = VLM(vision_encoder, text_decoder)\n",
        "\n",
        "    # Load projection layer\n",
        "    vlm_model.projection.load_state_dict(torch.load(f\"{path}/projection_layer.pt\"))\n",
        "\n",
        "    return vlm_model, tokenizer\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Login to wandb\n",
        "    try:\n",
        "        wandb.login()\n",
        "    except:\n",
        "        logger.warning(\"Could not log in to Weights & Biases. Continuing without logging.\")\n",
        "\n",
        "    # Set up transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = VLMDataset(folder_path=\"dataset\", transform=transform)\n",
        "    logger.info(f\"Dataset size: {len(dataset)} image-text pairs\")\n",
        "\n",
        "    # Split dataset: 80% train, 20% eval\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    eval_size = len(dataset) - train_size\n",
        "    train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, eval_size])\n",
        "\n",
        "    # Create dataloaders\n",
        "    batch_size = 8  # Increased batch size since we're using less memory with QLoRA\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize models\n",
        "    vlm_model, tokenizer = initialize_models()\n",
        "\n",
        "    # Initialize optimizer - we only train the projection layer and LoRA weights\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': vlm_model.projection.parameters(), 'lr': 1e-4},\n",
        "        {'params': vlm_model.text_decoder.parameters(), 'lr': 5e-5}\n",
        "    ])\n",
        "\n",
        "    # Train the model\n",
        "    logger.info(\"Starting VLM training...\")\n",
        "    vlm_model = train_vlm(vlm_model, train_loader, optimizer, tokenizer, epochs=5)\n",
        "\n",
        "    # Save final model efficiently\n",
        "    save_efficient_model(vlm_model, tokenizer, path=\"efficient_vlm_final\")\n",
        "    logger.info(\"Efficient model saved to efficient_vlm_final\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    logger.info(\"Evaluating VLM...\")\n",
        "    evaluate_vlm(vlm_model, tokenizer, eval_dataset, num_samples=5)\n",
        "\n",
        "    # Demonstrate loading the efficient model\n",
        "    loaded_model, loaded_tokenizer = load_efficient_model(path=\"efficient_vlm_final\")\n",
        "    logger.info(\"Successfully loaded the efficient model\")\n",
        "\n",
        "    logger.info(\"Training and evaluation complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "HKf5HmyODgj0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}